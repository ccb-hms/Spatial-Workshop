{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Integrating Multiple Modalities\n",
    "\n",
    "**Tutor:** Anthony Christidis\n",
    "**Time:** 30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "In our final notebook, we will tackle one of the most powerful and scientifically relevant challenges in spatial biology: integrating data from multiple technologies. We have two datasets from the same block of human breast cancer tissue:\n",
    "\n",
    "1. A **Visium** dataset, providing a whole-transcriptome view at spot-level resolution.\n",
    "2. A **Xenium** dataset, providing a high-resolution view of ~300 targeted genes at the single-cell level.\n",
    "\n",
    "Our goal is to align these two datasets into a single, shared coordinate system. This will allow us to, for example, analyze the full transcriptome of Visium spots that fall within a specific high-resolution cell type niche defined by Xenium.\n",
    "\n",
    "**Goals:**\n",
    "1. Understand the concept of coordinate systems in `SpatialData`.\n",
    "2. Use `napari` to interactively define corresponding landmarks on two different images.\n",
    "3. Apply a transformation to align the datasets into a common coordinate space.\n",
    "4. Use `sd.aggregate()` to summarize high-resolution data within low-resolution regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Datasets\n",
    "\n",
    "First, let's load our two datasets. These are subsets of the data used in the original `nbXX` notebook, cropped to make them manageable for the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black\n",
    "\n",
    "import spatialdata as sd\n",
    "import napari_spatialdata as nsd\n",
    "from spatialdata.transformations import get_transformation_between_landmarks, set_transformation\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_path = \"../data/\"\n",
    "\n",
    "sdata_visium = sd.read_zarr(data_path + \"visium_breast_cancer_subset.zarr\")\n",
    "sdata_xenium = sd.read_zarr(data_path + \"xenium_breast_cancer_subset.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Visium Data ---\")\n",
    "print(sdata_visium)\n",
    "print(\"\\n--- Xenium Data ---\")\n",
    "print(sdata_xenium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Interactive Landmark-Based Alignment\n",
    "\n",
    "The two datasets are currently in their own separate coordinate systems (`global`). To align them, we need to find common features in both images and mark them as landmarks. `napari` is the perfect tool for this.\n",
    "\n",
    "**Instructions:**\n",
    "1. Run the cell below to launch `napari` with both `SpatialData` objects.\n",
    "2. In the `napari` viewer, you will see two images. Find 3-4 features (like tissue boundaries or specific ducts) that are clearly visible in both the Visium and Xenium images.\n",
    "3. Add a new `Points` layer for each dataset by clicking the 'Add points layer' button.\n",
    "4. Carefully click on the same 3-4 landmark locations in each image, adding a point in the corresponding points layer.\n",
    "5. **Important:** Make sure you add the points in the same order for both images!\n",
    "6. Leave `napari` open and proceed to the next cell to extract the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This launches napari with both datasets loaded\n",
    "viewer = nsd.Interactive([sdata_visium, sdata_xenium])\n",
    "\n",
    "# After placing your landmarks, run the cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of what the landmark selection process looks like:\n",
    "\n",
    "![Landmark selection GIF](https://www.dropbox.com/s/z9d3d3b7v5d7s9t/CleanShot%202023-11-13%20at%2016.37.43.gif?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating and Applying the Transformation\n",
    "\n",
    "Once we have our landmarks, we can extract their coordinates from the `viewer` object and calculate the transformation matrix needed to align the Visium data to the Xenium data's coordinate space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Layer names might differ slightly. Adjust if necessary.\n",
    "# Let's assume you created 'Points' and 'Points [1]' layers.\n",
    "visium_landmark_coords = viewer.window.viewer.layers['Points'].data\n",
    "xenium_landmark_coords = viewer.window.viewer.layers['Points [1]'].data\n",
    "\n",
    "# Calculate the affine transformation\n",
    "affine_transform = get_transformation_between_landmarks(\n",
    "    moving_coords=visium_landmark_coords,\n",
    "    references_coords=xenium_landmark_coords\n",
    ")\n",
    "\n",
    "# Apply this transformation to all elements in the Visium object,\n",
    "# creating a new 'aligned_to_xenium' coordinate system.\n",
    "set_transformation(\n",
    "    sdata_visium.images['CytAssist_FFPE_Human_Breast_Cancer_full_image'],\n",
    "    affine_transform, \n",
    "    \"aligned_to_xenium\"\n",
    ")\n",
    "set_transformation(\n",
    "    sdata_visium.shapes['CytAssist_FFPE_Human_Breast_Cancer'],\n",
    "    affine_transform, \n",
    "    \"aligned_to_xenium\"\n",
    ")\n",
    "\n",
    "print(\"Transformation applied!\")\n",
    "viewer.window.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Aligned Result\n",
    "\n",
    "Let's use `spatialdata-plot` to visualize our success. We will render the Xenium image and overlay the newly transformed Visium spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata_xenium.pl.render_images().pl.show(figsize=(8,8), title=\"Xenium Image\")\n",
    "\n",
    "# Now render the Visium spots in their new, aligned coordinate system\n",
    "sdata_visium.pl.render_shapes(\n",
    "    coordinate_systems=\"aligned_to_xenium\", \n",
    "    outline=True, \n",
    "    fill_alpha=0.1\n",
    ").pl.show(figsize=(8,8), title=\"Visium Spots Aligned to Xenium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Step: Aggregating Data Across Modalities\n",
    "\n",
    "Now that the data is aligned, we can perform powerful cross-modality analyses. As a final example, let's use `sd.aggregate()` to count how many high-resolution Xenium cells fall within each low-resolution Visium spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_counts_per_spot = sd.aggregate(\n",
    "    values=sdata_xenium.shapes['cell_boundaries'],\n",
    "    by=sdata_visium.shapes['CytAssist_FFPE_Human_Breast_Cancer'],\n",
    "    agg_func=\"count\",\n",
    ")\n",
    "\n",
    "# This adds a new column to our Visium table with the cell counts\n",
    "sdata_visium.tables[\"table\"].obs[\"xenium_cell_count\"] = cell_counts_per_spot[\"table\"].obs[\"agg_values\"]\n",
    "\n",
    "sdata_visium.tables[\"table\"].obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata_visium.pl.render_shapes(\n",
    "    color=\"xenium_cell_count\",\n",
    "    size=250,\n",
    ").pl.show(coordinate_systems=\"aligned_to_xenium\", title=\"Number of Xenium Cells per Visium Spot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workshop Conclusion\n",
    "\n",
    "Congratulations! Over the last three hours, you have learned a complete workflow:\n",
    "\n",
    "1.  How to load and explore complex spatial data with `SpatialData` and `napari`.\n",
    "2.  How to identify cell types with `scanpy` and analyze their organization with `squidpy`.\n",
    "3.  How to integrate multiple datasets from different technologies.\n",
    "\n",
    "You are now equipped with the foundational skills to start applying these powerful `scverse` tools to your own research questions. Thank you for participating!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
